{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31615938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud Transaction Detection Analysis\n",
    "\n",
    "## 1. Introduction\n",
    "# This project aims to detect fraudulent transactions using machine learning techniques.\n",
    "# The dataset contains simulated transaction data with three types of fraud patterns.\n",
    "\n",
    "## 2. Data Loading and Exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../data/raw/transactions.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "# Convert datetime column\n",
    "df['TX_DATETIME'] = pd.to_datetime(df['TX_DATETIME'])\n",
    "\n",
    "# Extract time-based features\n",
    "df['Hour'] = df['TX_DATETIME'].dt.hour\n",
    "df['DayOfWeek'] = df['TX_DATETIME'].dt.dayofweek\n",
    "df['Day'] = df['TX_DATETIME'].dt.day\n",
    "\n",
    "# Display processed data\n",
    "df.head()\n",
    "\n",
    "## 4. Feature Engineering\n",
    "# Create features based on domain knowledge\n",
    "\n",
    "# High amount flag (scenario 1)\n",
    "df['HIGH_AMOUNT'] = (df['TX_AMOUNT'] > 220).astype(int)\n",
    "\n",
    "# Customer behavior features\n",
    "customer_avg_amount = df.groupby('CUSTOMER_ID')['TX_AMOUNT'].mean().reset_index()\n",
    "customer_avg_amount.columns = ['CUSTOMER_ID', 'CUST_AVG_AMOUNT']\n",
    "df = df.merge(customer_avg_amount, on='CUSTOMER_ID', how='left')\n",
    "\n",
    "# Deviation from customer's average amount\n",
    "df['AMOUNT_DEVIATION'] = abs(df['TX_AMOUNT'] - df['CUST_AVG_AMOUNT']) / df['CUST_AVG_AMOUNT']\n",
    "\n",
    "# Terminal risk features (scenario 2)\n",
    "terminal_fraud_count = df.groupby('TERMINAL_ID')['TX_FRAUD'].sum().reset_index()\n",
    "terminal_fraud_count.columns = ['TERMINAL_ID', 'TERMINAL_FRAUD_COUNT']\n",
    "df = df.merge(terminal_fraud_count, on='TERMINAL_ID', how='left')\n",
    "\n",
    "# Customer risk features (scenario 3)\n",
    "customer_fraud_count = df.groupby('CUSTOMER_ID')['TX_FRAUD'].sum().reset_index()\n",
    "customer_fraud_count.columns = ['CUSTOMER_ID', 'CUSTOMER_FRAUD_COUNT']\n",
    "df = df.merge(customer_fraud_count, on='CUSTOMER_ID', how='left')\n",
    "\n",
    "# Time since last transaction for each customer\n",
    "df = df.sort_values(by=['CUSTOMER_ID', 'TX_DATETIME'])\n",
    "df['TIME_SINCE_LAST_TX'] = df.groupby('CUSTOMER_ID')['TX_DATETIME'].diff().dt.total_seconds() / 60\n",
    "df['TIME_SINCE_LAST_TX'].fillna(24*60, inplace=True)\n",
    "\n",
    "# Display engineered features\n",
    "df[['TX_AMOUNT', 'HIGH_AMOUNT', 'CUST_AVG_AMOUNT', 'AMOUNT_DEVIATION', \n",
    "    'TERMINAL_FRAUD_COUNT', 'CUSTOMER_FRAUD_COUNT', 'TIME_SINCE_LAST_TX']].head()\n",
    "\n",
    "## 5. Data Visualization\n",
    "# Distribution of target variable\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='TX_FRAUD', data=df)\n",
    "plt.title('Distribution of Fraudulent Transactions')\n",
    "plt.xlabel('Fraud Status (0: Legitimate, 1: Fraudulent)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Transaction amount distribution by fraud status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[df['TX_FRAUD'] == 0]['TX_AMOUNT'], bins=50, color='green', alpha=0.6, label='Legitimate')\n",
    "sns.histplot(df[df['TX_FRAUD'] == 1]['TX_AMOUNT'], bins=50, color='red', alpha=0.6, label='Fraudulent')\n",
    "plt.legend()\n",
    "plt.title('Transaction Amount Distribution by Fraud Status')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Fraud rate by hour of day\n",
    "hourly_fraud_rate = df.groupby('Hour')['TX_FRAUD'].mean()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_fraud_rate.index, hourly_fraud_rate.values, marker='o')\n",
    "plt.title('Hourly Fraud Rate')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Fraud Rate')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "features = ['TX_AMOUNT', 'HIGH_AMOUNT', 'DayOfWeek', 'Hour', \n",
    "            'CUST_AVG_AMOUNT', 'AMOUNT_DEVIATION', \n",
    "            'TERMINAL_FRAUD_COUNT', 'CUSTOMER_FRAUD_COUNT', \n",
    "            'TIME_SINCE_LAST_TX', 'TX_FRAUD']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "## 6. Data Preparation for Modeling\n",
    "# Select features for modeling\n",
    "features = ['TX_AMOUNT', 'HIGH_AMOUNT', 'DayOfWeek', 'Hour', \n",
    "            'CUST_AVG_AMOUNT', 'AMOUNT_DEVIATION', \n",
    "            'TERMINAL_FRAUD_COUNT', 'CUSTOMER_FRAUD_COUNT', \n",
    "            'TIME_SINCE_LAST_TX']\n",
    "target = 'TX_FRAUD'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "print(f\"Resampled dataset shape: {X_res.shape}\")\n",
    "print(f\"Class distribution after SMOTE: {pd.Series(y_res).value_counts()}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "\n",
    "## 7. Model Training\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = model.score(X_test_scaled, y_test)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_roc': auc_roc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Find the best model based on AUC-ROC\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc_roc'])\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\nBest model: {best_model_name} with AUC-ROC: {results[best_model_name]['auc_roc']:.4f}\")\n",
    "\n",
    "## 8. Model Evaluation\n",
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {result[\"auc_roc\"]:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Different Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for the best model\n",
    "best_result = results[best_model_name]\n",
    "cm = confusion_matrix(y_test, best_result['y_pred'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraudulent'], \n",
    "            yticklabels=['Legitimate', 'Fraudulent'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, result in results.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test, result['y_pred_proba'])\n",
    "    plt.plot(recall, precision, label=name)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## 9. Business Impact Analysis\n",
    "# Calculate potential savings from fraud detection\n",
    "df['PREDICTED_FRAUD'] = best_model.predict(scaler.transform(X))\n",
    "df['FRAUD_PROBABILITY'] = best_model.predict_proba(scaler.transform(X))[:, 1]\n",
    "\n",
    "# Calculate potential savings\n",
    "true_frauds = df[(df['TX_FRAUD'] == 1) & (df['PREDICTED_FRAUD'] == 1)]\n",
    "false_positives = df[(df['TX_FRAUD'] == 0) & (df['PREDICTED_FRAUD'] == 1)]\n",
    "false_negatives = df[(df['TX_FRAUD'] == 1) & (df['PREDICTED_FRAUD'] == 0)]\n",
    "\n",
    "total_fraud_amount = df[df['TX_FRAUD'] == 1]['TX_AMOUNT'].sum()\n",
    "detected_fraud_amount = true_frauds['TX_AMOUNT'].sum()\n",
    "missed_fraud_amount = false_negatives['TX_AMOUNT'].sum()\n",
    "false_positive_amount = false_positives['TX_AMOUNT'].sum()\n",
    "\n",
    "print(f\"Total fraud amount: ${total_fraud_amount:,.2f}\")\n",
    "print(f\"Detected fraud amount: ${detected_fraud_amount:,.2f} ({detected_fraud_amount/total_fraud_amount*100:.2f}%)\")\n",
    "print(f\"Missed fraud amount: ${missed_fraud_amount:,.2f} ({missed_fraud_amount/total_fraud_amount*100:.2f}%)\")\n",
    "print(f\"False positive amount: ${false_positive_amount:,.2f}\")\n",
    "\n",
    "# Calculate precision and recall for business context\n",
    "precision = len(true_frauds) / (len(true_frauds) + len(false_positives))\n",
    "recall = len(true_frauds) / (len(true_frauds) + len(false_negatives))\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.4f} (Percentage of detected frauds that are actual frauds)\")\n",
    "print(f\"Recall: {recall:.4f} (Percentage of actual frauds that are detected)\")\n",
    "\n",
    "## 10. Conclusion and Next Steps\n",
    "# Summary of findings\n",
    "print(\"Key Findings:\")\n",
    "print(f\"1. Best performing model: {best_model_name}\")\n",
    "print(f\"2. AUC-ROC score: {results[best_model_name]['auc_roc']:.4f}\")\n",
    "print(f\"3. Most important features: {feature_importance['feature'].head(3).tolist()}\")\n",
    "print(f\"4. Business impact: Could prevent ${detected_fraud_amount:,.2f} in fraudulent transactions\")\n",
    "\n",
    "# Recommendations for deployment\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Implement real-time monitoring of high-risk transactions\")\n",
    "print(\"2. Set up alerts for transactions with high fraud probability\")\n",
    "print(\"3. Regularly update the model with new transaction data\")\n",
    "print(\"4. Combine machine learning with rule-based systems for better coverage\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Deploy the model as an API for real-time predictions\")\n",
    "print(\"2. Implement a feedback loop to improve model performance\")\n",
    "print(\"3. Explore deep learning approaches for improved detection\")\n",
    "print(\"4. Add more features like geolocation and merchant category\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
